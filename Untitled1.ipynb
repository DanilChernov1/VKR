{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7ea50fed-b286-4395-9cc1-473f17999a67",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-27T21:14:52.351302Z",
     "iopub.status.busy": "2024-11-27T21:14:52.350320Z",
     "iopub.status.idle": "2024-11-27T21:19:55.317066Z",
     "shell.execute_reply": "2024-11-27T21:19:55.316175Z",
     "shell.execute_reply.started": "2024-11-27T21:14:52.351256Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device:  cpu\n",
      "Start from checkpoint: results/last_mdx23c.ckpt\n",
      "Instruments: ['vocals', 'other']\n",
      "Model load time: 2.16 sec\n",
      "Total files found: 1 Use sample rate: 44100\n",
      "Starting processing track:  input/wavs/1.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 300.78 sec\n"
     ]
    }
   ],
   "source": [
    "%run -i inferenceGPU.py  --model_type mdx23c  --config_path configs/config_vocals_mdx23c.yaml  --start_check_point results/last_mdx23c.ckpt  --input_folder input/wavs/  --store_dir separation_results/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b48ed768-456a-4e5c-835c-a587e8aed52e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-25T07:07:10.033968Z",
     "iopub.status.busy": "2024-11-25T07:07:10.032450Z",
     "iopub.status.idle": "2024-11-25T07:07:10.085986Z",
     "shell.execute_reply": "2024-11-25T07:07:10.084698Z",
     "shell.execute_reply.started": "2024-11-25T07:07:10.033927Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 4, 4096, 256)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1, 4, 4096, 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "923faf2e-a658-4b6d-986f-b036621abf59",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-27T21:19:55.319256Z",
     "iopub.status.busy": "2024-11-27T21:19:55.318704Z",
     "iopub.status.idle": "2024-11-27T21:19:56.012609Z",
     "shell.execute_reply": "2024-11-27T21:19:56.011840Z",
     "shell.execute_reply.started": "2024-11-27T21:19:55.319223Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model, config = get_model_from_config('mdx23c', 'configs/config_vocals_mdx23c.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2cbf4bfa-235f-4e69-89c0-523372ee92c8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-25T07:07:11.098697Z",
     "iopub.status.busy": "2024-11-25T07:07:11.097212Z",
     "iopub.status.idle": "2024-11-25T07:07:11.133209Z",
     "shell.execute_reply": "2024-11-25T07:07:11.132006Z",
     "shell.execute_reply.started": "2024-11-25T07:07:11.098645Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def export_model_to_onnx(model, input_tensor, onnx_output_file):\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    print(input_tensor.dtype)\n",
    "    # Экспорт модели в ONNX\n",
    "    torch.onnx.export(\n",
    "        model, \n",
    "        input_tensor, \n",
    "        onnx_output_file,\n",
    "        export_params=True,\n",
    "        verbose = True\n",
    "    )\n",
    "    print(f\"Model has been exported to {onnx_output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d877419c-24b2-405e-b660-6eb2d59e5360",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-27T21:12:44.797755Z",
     "iopub.status.busy": "2024-11-27T21:12:44.797036Z",
     "iopub.status.idle": "2024-11-27T21:12:44.845029Z",
     "shell.execute_reply": "2024-11-27T21:12:44.844241Z",
     "shell.execute_reply.started": "2024-11-27T21:12:44.797719Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2822/278296771.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0minput_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4096\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m256\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mexport_model_to_onnx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"mdx23c.onnx\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "input_tensor = torch.randn(1, 4, 4096, 256)\n",
    "export_model_to_onnx(model, input_tensor, \"mdx23c.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "255d48e1-72b0-4631-87de-298da1674019",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-27T21:19:56.014367Z",
     "iopub.status.busy": "2024-11-27T21:19:56.013529Z",
     "iopub.status.idle": "2024-11-27T21:19:57.285501Z",
     "shell.execute_reply": "2024-11-27T21:19:57.284678Z",
     "shell.execute_reply.started": "2024-11-27T21:19:56.014337Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device:  cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import onnxruntime as ort\n",
    "device = 'cpu'\n",
    "if torch.cuda.is_available():\n",
    "    print('CUDA is available, use --force_cpu to disable it.')\n",
    "    device = \"cuda\"\n",
    "\n",
    "print(\"Using device: \", device)\n",
    "\n",
    "providers = ['CUDAExecutionProvider', 'CPUExecutionProvider'] if device.startswith('cuda') else ['CPUExecutionProvider']\n",
    "ort_session = ort.InferenceSession('mdx23c.onnx', providers=providers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "907f719d-8722-4bd9-899f-e999f251aa25",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-27T21:19:57.287583Z",
     "iopub.status.busy": "2024-11-27T21:19:57.286939Z",
     "iopub.status.idle": "2024-11-27T21:19:57.317328Z",
     "shell.execute_reply": "2024-11-27T21:19:57.316752Z",
     "shell.execute_reply.started": "2024-11-27T21:19:57.287559Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from ml_collections import ConfigDict\n",
    "import yaml\n",
    "from models.mdx23c_tfc_tdf_v3 import STFT\n",
    "file = open(\"/home/jupyter/datasphere/project/Music-Source-Separation-Training/configs/config_vocals_mdx23c.yaml\", \"r\")\n",
    "config = ConfigDict(yaml.load(file, Loader=yaml.FullLoader))\n",
    "file.close\n",
    "con = STFT(config.audio)\n",
    "\n",
    "def infer_onnx(input_batch):\n",
    "    # Конвертируем входные данные в формат numpy\n",
    "    input_numpy = input_batch.numpy()\n",
    "    \n",
    "    # Предполагаем, что ваша модель требует только один входной массив\n",
    "    ort_inputs = {ort_session.get_inputs()[0].name: input_numpy}\n",
    "    \n",
    "    # Выполнение предсказания\n",
    "    ort_outs = ort_session.run(None, ort_inputs)\n",
    "    \n",
    "    # Возвращаем первый элемент списка выходов, если он единственный\n",
    "    return ort_outs[0]\n",
    "\n",
    "def _getWindowingArray(window_size, fade_size):\n",
    "    fadein = torch.linspace(0, 1, fade_size)\n",
    "    fadeout = torch.linspace(1, 0, fade_size)\n",
    "    window = torch.ones(window_size)\n",
    "    window[-fade_size:] *= fadeout\n",
    "    window[:fade_size] *= fadein\n",
    "    return window\n",
    "\n",
    "def my_demix_track(config, model, mix, device, pbar=False):\n",
    "    C = config.audio.chunk_size\n",
    "    N = config.inference.num_overlap\n",
    "    fade_size = C // 10\n",
    "    step = int(C // N)\n",
    "    border = C - step\n",
    "    batch_size = config.inference.batch_size\n",
    "    mix = torch.tensor(mix)\n",
    "    length_init = mix.shape[-1]\n",
    "\n",
    "    # Do pad from the beginning and end to account floating window results better\n",
    "    if length_init > 2 * border and (border > 0):\n",
    "        mix = nn.functional.pad(mix, (border, border), mode='reflect')\n",
    "\n",
    "    # windowingArray crossfades at segment boundaries to mitigate clicking artifacts\n",
    "    windowingArray = _getWindowingArray(C, fade_size)\n",
    "\n",
    "    with torch.cuda.amp.autocast(enabled=config.training.use_amp):\n",
    "        with torch.inference_mode():\n",
    "            req_shape = (len(prefer_target_instrument(config)),) + tuple(mix.shape)\n",
    "\n",
    "            result = torch.zeros(req_shape, dtype=torch.float32)\n",
    "            counter = torch.zeros(req_shape, dtype=torch.float32)\n",
    "            i = 0\n",
    "            batch_data = []\n",
    "            batch_locations = []\n",
    "            progress_bar = tqdm(total=mix.shape[1], desc=\"Processing audio chunks\", leave=False) if pbar else None\n",
    "\n",
    "            while i < mix.shape[1]:\n",
    "                # print(i, i + C, mix.shape[1])\n",
    "                part = mix[:, i:i + C].to(device)\n",
    "                length = part.shape[-1]\n",
    "                if length < C:\n",
    "                    if length > C // 2 + 1:\n",
    "                        part = nn.functional.pad(input=part, pad=(0, C - length), mode='reflect')\n",
    "                    else:\n",
    "                        part = nn.functional.pad(input=part, pad=(0, C - length, 0, 0), mode='constant', value=0)\n",
    "                batch_data.append(part)\n",
    "                batch_locations.append((i, length))\n",
    "                i += step\n",
    "\n",
    "                if len(batch_data) >= batch_size or (i >= mix.shape[1]):\n",
    "                    arr = torch.stack(batch_data, dim=0)\n",
    "                    \n",
    "                    x = con(arr).cpu()\n",
    "                    x = torch.tensor(infer_onnx(x)).to(device)\n",
    "                    x = con.inverse(x)\n",
    "                    \n",
    "\n",
    "                    window = windowingArray\n",
    "                    if i - step == 0:  # First audio chunk, no fadein\n",
    "                        window[:fade_size] = 1\n",
    "                    elif i >= mix.shape[1]:  # Last audio chunk, no fadeout\n",
    "                        window[-fade_size:] = 1\n",
    "\n",
    "                    for j in range(len(batch_locations)):\n",
    "                        start, l = batch_locations[j]\n",
    "                        result[..., start:start+l] += x[j][..., :l].cpu() * window[..., :l]\n",
    "                        counter[..., start:start+l] += window[..., :l]\n",
    "\n",
    "                    batch_data = []\n",
    "                    batch_locations = []\n",
    "\n",
    "                if progress_bar:\n",
    "                    progress_bar.update(step)\n",
    "\n",
    "            if progress_bar:\n",
    "                progress_bar.close()\n",
    "\n",
    "            estimated_sources = result / counter\n",
    "            estimated_sources = estimated_sources.cpu().numpy()\n",
    "            np.nan_to_num(estimated_sources, copy=False, nan=0.0)\n",
    "\n",
    "            if length_init > 2 * border and (border > 0):\n",
    "                # Remove pad\n",
    "                estimated_sources = estimated_sources[..., border:-border]\n",
    "\n",
    "    return {k: v for k, v in zip(prefer_target_instrument(config), estimated_sources)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a27a6746-34eb-4dbd-a4b4-cc0089bb5bc7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-27T21:19:57.318747Z",
     "iopub.status.busy": "2024-11-27T21:19:57.318138Z",
     "iopub.status.idle": "2024-11-27T21:23:48.943140Z",
     "shell.execute_reply": "2024-11-27T21:23:48.942237Z",
     "shell.execute_reply.started": "2024-11-27T21:19:57.318716Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total files found: 1 Use sample rate: 44100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Total progress:   0%|          | 0/1 [00:00<?, ?it/s, track=1.wav]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting processing track:  /home/jupyter/datasphere/project/Music-Source-Separation-Training/input/wavs/1.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing audio chunks:   0%|          | 0/5861376 [00:00<?, ?it/s]\u001b[A\n",
      "Processing audio chunks:   1%|          | 65280/5861376 [00:03<04:51, 19857.69it/s]\u001b[A\n",
      "Processing audio chunks:   2%|▏         | 130560/5861376 [00:06<04:43, 20234.36it/s]\u001b[A\n",
      "Processing audio chunks:   3%|▎         | 195840/5861376 [00:09<04:23, 21467.87it/s]\u001b[A\n",
      "Processing audio chunks:   4%|▍         | 261120/5861376 [00:12<04:18, 21655.45it/s]\u001b[A\n",
      "Processing audio chunks:   6%|▌         | 326400/5861376 [00:15<04:17, 21492.04it/s]\u001b[A\n",
      "Processing audio chunks:   7%|▋         | 391680/5861376 [00:17<03:56, 23088.91it/s]\u001b[A\n",
      "Processing audio chunks:   8%|▊         | 456960/5861376 [00:20<03:57, 22796.61it/s]\u001b[A\n",
      "Processing audio chunks:   9%|▉         | 522240/5861376 [00:23<03:47, 23517.29it/s]\u001b[A\n",
      "Processing audio chunks:  10%|█         | 587520/5861376 [00:25<03:39, 24028.31it/s]\u001b[A\n",
      "Processing audio chunks:  11%|█         | 652800/5861376 [00:28<03:30, 24700.15it/s]\u001b[A\n",
      "Processing audio chunks:  12%|█▏        | 718080/5861376 [00:30<03:21, 25468.91it/s]\u001b[A\n",
      "Processing audio chunks:  13%|█▎        | 783360/5861376 [00:33<03:15, 25956.98it/s]\u001b[A\n",
      "Processing audio chunks:  14%|█▍        | 848640/5861376 [00:35<03:12, 26041.99it/s]\u001b[A\n",
      "Processing audio chunks:  16%|█▌        | 913920/5861376 [00:38<03:10, 26016.68it/s]\u001b[A\n",
      "Processing audio chunks:  17%|█▋        | 979200/5861376 [00:40<03:12, 25386.61it/s]\u001b[A\n",
      "Processing audio chunks:  18%|█▊        | 1044480/5861376 [00:43<03:10, 25315.13it/s]\u001b[A\n",
      "Processing audio chunks:  19%|█▉        | 1109760/5861376 [00:46<03:08, 25228.08it/s]\u001b[A\n",
      "Processing audio chunks:  20%|██        | 1175040/5861376 [00:48<03:02, 25633.64it/s]\u001b[A\n",
      "Processing audio chunks:  21%|██        | 1240320/5861376 [00:51<02:59, 25799.43it/s]\u001b[A\n",
      "Processing audio chunks:  22%|██▏       | 1305600/5861376 [00:53<02:54, 26152.70it/s]\u001b[A\n",
      "Processing audio chunks:  23%|██▎       | 1370880/5861376 [00:55<02:50, 26409.99it/s]\u001b[A\n",
      "Processing audio chunks:  25%|██▍       | 1436160/5861376 [00:58<02:47, 26477.33it/s]\u001b[A\n",
      "Processing audio chunks:  26%|██▌       | 1501440/5861376 [01:00<02:43, 26693.23it/s]\u001b[A\n",
      "Processing audio chunks:  27%|██▋       | 1566720/5861376 [01:03<02:40, 26794.12it/s]\u001b[A\n",
      "Processing audio chunks:  28%|██▊       | 1632000/5861376 [01:05<02:38, 26612.89it/s]\u001b[A\n",
      "Processing audio chunks:  29%|██▉       | 1697280/5861376 [01:08<02:37, 26366.94it/s]\u001b[A\n",
      "Processing audio chunks:  30%|███       | 1762560/5861376 [01:10<02:35, 26442.85it/s]\u001b[A\n",
      "Processing audio chunks:  31%|███       | 1827840/5861376 [01:13<02:32, 26465.70it/s]\u001b[A\n",
      "Processing audio chunks:  32%|███▏      | 1893120/5861376 [01:15<02:29, 26534.00it/s]\u001b[A\n",
      "Processing audio chunks:  33%|███▎      | 1958400/5861376 [01:17<02:26, 26694.18it/s]\u001b[A\n",
      "Processing audio chunks:  35%|███▍      | 2023680/5861376 [01:20<02:23, 26744.35it/s]\u001b[A\n",
      "Processing audio chunks:  36%|███▌      | 2088960/5861376 [01:22<02:20, 26834.51it/s]\u001b[A\n",
      "Processing audio chunks:  37%|███▋      | 2154240/5861376 [01:25<02:18, 26847.93it/s]\u001b[A\n",
      "Processing audio chunks:  38%|███▊      | 2219520/5861376 [01:27<02:15, 26937.33it/s]\u001b[A\n",
      "Processing audio chunks:  39%|███▉      | 2284800/5861376 [01:29<02:12, 26997.00it/s]\u001b[A\n",
      "Processing audio chunks:  40%|████      | 2350080/5861376 [01:32<02:11, 26738.16it/s]\u001b[A\n",
      "Processing audio chunks:  41%|████      | 2415360/5861376 [01:34<02:09, 26574.73it/s]\u001b[A\n",
      "Processing audio chunks:  42%|████▏     | 2480640/5861376 [01:37<02:08, 26256.59it/s]\u001b[A\n",
      "Processing audio chunks:  43%|████▎     | 2545920/5861376 [01:39<02:05, 26318.58it/s]\u001b[A\n",
      "Processing audio chunks:  45%|████▍     | 2611200/5861376 [01:42<02:02, 26448.56it/s]\u001b[A\n",
      "Processing audio chunks:  46%|████▌     | 2676480/5861376 [01:44<02:00, 26501.03it/s]\u001b[A\n",
      "Processing audio chunks:  47%|████▋     | 2741760/5861376 [01:47<01:57, 26663.24it/s]\u001b[A\n",
      "Processing audio chunks:  48%|████▊     | 2807040/5861376 [01:49<01:54, 26778.23it/s]\u001b[A\n",
      "Processing audio chunks:  49%|████▉     | 2872320/5861376 [01:52<01:51, 26726.90it/s]\u001b[A\n",
      "Processing audio chunks:  50%|█████     | 2937600/5861376 [01:54<01:49, 26803.01it/s]\u001b[A\n",
      "Processing audio chunks:  51%|█████     | 3002880/5861376 [01:56<01:46, 26928.87it/s]\u001b[A\n",
      "Processing audio chunks:  52%|█████▏    | 3068160/5861376 [01:59<01:43, 27030.69it/s]\u001b[A\n",
      "Processing audio chunks:  53%|█████▎    | 3133440/5861376 [02:01<01:40, 27074.15it/s]\u001b[A\n",
      "Processing audio chunks:  55%|█████▍    | 3198720/5861376 [02:04<01:38, 27046.34it/s]\u001b[A\n",
      "Processing audio chunks:  56%|█████▌    | 3264000/5861376 [02:06<01:36, 27051.81it/s]\u001b[A\n",
      "Processing audio chunks:  57%|█████▋    | 3329280/5861376 [02:09<01:33, 27059.09it/s]\u001b[A\n",
      "Processing audio chunks:  58%|█████▊    | 3394560/5861376 [02:11<01:31, 27038.86it/s]\u001b[A\n",
      "Processing audio chunks:  59%|█████▉    | 3459840/5861376 [02:13<01:28, 27040.87it/s]\u001b[A\n",
      "Processing audio chunks:  60%|██████    | 3525120/5861376 [02:16<01:26, 27058.11it/s]\u001b[A\n",
      "Processing audio chunks:  61%|██████▏   | 3590400/5861376 [02:19<01:32, 24674.20it/s]\u001b[A\n",
      "Processing audio chunks:  62%|██████▏   | 3655680/5861376 [02:21<01:27, 25346.37it/s]\u001b[A\n",
      "Processing audio chunks:  63%|██████▎   | 3720960/5861376 [02:24<01:23, 25779.49it/s]\u001b[A\n",
      "Processing audio chunks:  65%|██████▍   | 3786240/5861376 [02:26<01:19, 26165.62it/s]\u001b[A\n",
      "Processing audio chunks:  66%|██████▌   | 3851520/5861376 [02:29<01:16, 26411.65it/s]\u001b[A\n",
      "Processing audio chunks:  67%|██████▋   | 3916800/5861376 [02:31<01:13, 26555.63it/s]\u001b[A\n",
      "Processing audio chunks:  68%|██████▊   | 3982080/5861376 [02:33<01:10, 26607.20it/s]\u001b[A\n",
      "Processing audio chunks:  69%|██████▉   | 4047360/5861376 [02:36<01:08, 26518.04it/s]\u001b[A\n",
      "Processing audio chunks:  70%|███████   | 4112640/5861376 [02:38<01:06, 26342.58it/s]\u001b[A\n",
      "Processing audio chunks:  71%|███████▏  | 4177920/5861376 [02:41<01:04, 26272.85it/s]\u001b[A\n",
      "Processing audio chunks:  72%|███████▏  | 4243200/5861376 [02:44<01:01, 26154.44it/s]\u001b[A\n",
      "Processing audio chunks:  74%|███████▎  | 4308480/5861376 [02:46<00:59, 26231.99it/s]\u001b[A\n",
      "Processing audio chunks:  75%|███████▍  | 4373760/5861376 [02:48<00:56, 26384.55it/s]\u001b[A\n",
      "Processing audio chunks:  76%|███████▌  | 4439040/5861376 [02:51<00:53, 26596.67it/s]\u001b[A\n",
      "Processing audio chunks:  77%|███████▋  | 4504320/5861376 [02:53<00:50, 26641.69it/s]\u001b[A\n",
      "Processing audio chunks:  78%|███████▊  | 4569600/5861376 [02:56<00:52, 24384.88it/s]\u001b[A\n",
      "Processing audio chunks:  79%|███████▉  | 4634880/5861376 [02:59<00:48, 25143.30it/s]\u001b[A\n",
      "Processing audio chunks:  80%|████████  | 4700160/5861376 [03:01<00:45, 25702.76it/s]\u001b[A\n",
      "Processing audio chunks:  81%|████████▏ | 4765440/5861376 [03:04<00:42, 26057.29it/s]\u001b[A\n",
      "Processing audio chunks:  82%|████████▏ | 4830720/5861376 [03:06<00:39, 26326.09it/s]\u001b[A\n",
      "Processing audio chunks:  84%|████████▎ | 4896000/5861376 [03:09<00:36, 26462.83it/s]\u001b[A\n",
      "Processing audio chunks:  85%|████████▍ | 4961280/5861376 [03:11<00:33, 26643.09it/s]\u001b[A\n",
      "Processing audio chunks:  86%|████████▌ | 5026560/5861376 [03:13<00:31, 26773.51it/s]\u001b[A\n",
      "Processing audio chunks:  87%|████████▋ | 5091840/5861376 [03:16<00:29, 26363.59it/s]\u001b[A\n",
      "Processing audio chunks:  88%|████████▊ | 5157120/5861376 [03:18<00:26, 26527.18it/s]\u001b[A\n",
      "Processing audio chunks:  89%|████████▉ | 5222400/5861376 [03:21<00:23, 26670.05it/s]\u001b[A\n",
      "Processing audio chunks:  90%|█████████ | 5287680/5861376 [03:23<00:21, 26764.02it/s]\u001b[A\n",
      "Processing audio chunks:  91%|█████████▏| 5352960/5861376 [03:26<00:18, 26881.02it/s]\u001b[A\n",
      "Processing audio chunks:  92%|█████████▏| 5418240/5861376 [03:28<00:16, 26981.21it/s]\u001b[A\n",
      "Processing audio chunks:  94%|█████████▎| 5483520/5861376 [03:30<00:14, 26984.15it/s]\u001b[A\n",
      "Processing audio chunks:  95%|█████████▍| 5548800/5861376 [03:33<00:11, 27025.94it/s]\u001b[A\n",
      "Processing audio chunks:  96%|█████████▌| 5614080/5861376 [03:35<00:09, 27028.60it/s]\u001b[A\n",
      "Processing audio chunks:  97%|█████████▋| 5679360/5861376 [03:38<00:06, 27033.21it/s]\u001b[A\n",
      "Processing audio chunks:  98%|█████████▊| 5744640/5861376 [03:40<00:04, 27067.33it/s]\u001b[A\n",
      "Processing audio chunks:  99%|█████████▉| 5809920/5861376 [03:42<00:01, 27094.68it/s]\u001b[A\n",
      "Processing audio chunks: 5875200it [03:45, 27074.70it/s]                             \u001b[A\n",
      "Total progress: 100%|██████████| 1/1 [03:50<00:00, 230.59s/it, track=1.wav]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 231.60 sec\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import time\n",
    "import librosa\n",
    "from tqdm.auto import tqdm\n",
    "import sys\n",
    "import os\n",
    "import glob\n",
    "import torch\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "import torch.nn as nn\n",
    "from utils import prefer_target_instrument\n",
    "from models.mdx23c_tfc_tdf_v3 import STFT\n",
    "\n",
    "store_dir = \"separation_results/\"\n",
    "\n",
    "current_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "sys.path.append(current_dir)\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "all_mixtures_path = glob.glob(\"/home/jupyter/datasphere/project/Music-Source-Separation-Training/input/wavs\" + '/*.*')\n",
    "all_mixtures_path.sort()\n",
    "sample_rate = 44100\n",
    "if 'sample_rate' in config.audio:\n",
    "    sample_rate = config.audio['sample_rate']\n",
    "print('Total files found: {} Use sample rate: {}'.format(len(all_mixtures_path), sample_rate))\n",
    "\n",
    "instruments = prefer_target_instrument(config)\n",
    "verbose = False\n",
    "\n",
    "if not verbose:\n",
    "    all_mixtures_path = tqdm(all_mixtures_path, desc=\"Total progress\")\n",
    "\n",
    "if False:\n",
    "    detailed_pbar = False\n",
    "else:\n",
    "    detailed_pbar = True\n",
    "\n",
    "for path in all_mixtures_path:\n",
    "    print(\"Starting processing track: \", path)\n",
    "    if not verbose:\n",
    "        all_mixtures_path.set_postfix({'track': os.path.basename(path)})\n",
    "    try:\n",
    "        mix, sr = librosa.load(path, sr=sample_rate, mono=False)\n",
    "    except Exception as e:\n",
    "        print('Cannot read track: {}'.format(path))\n",
    "        print('Error message: {}'.format(str(e)))\n",
    "        continue\n",
    "\n",
    "    # Convert mono to stereo if needed\n",
    "    if len(mix.shape) == 1:\n",
    "        mix = np.stack([mix, mix], axis=0)\n",
    "\n",
    "    mix_orig = mix.copy()\n",
    "    if 'normalize' in config.inference:\n",
    "        if config.inference['normalize'] is True:\n",
    "            mono = mix.mean(0)\n",
    "            mean = mono.mean()\n",
    "            std = mono.std()\n",
    "            mix = (mix - mean) / std\n",
    "\n",
    "    if False:\n",
    "        # orig, channel inverse, polarity inverse\n",
    "        track_proc_list = [mix.copy(), mix[::-1].copy(), -1. * mix.copy()]\n",
    "    else:\n",
    "        track_proc_list = [mix.copy()]\n",
    "\n",
    "    full_result = []\n",
    "    for mix in track_proc_list:\n",
    "        waveforms = my_demix_track(config, model, mix, device, pbar=detailed_pbar)\n",
    "        mix = torch.tensor(mix, dtype=torch.float32)\n",
    "        \n",
    "        full_result.append(waveforms)\n",
    "\n",
    "    # Average all values in single dict\n",
    "    waveforms = full_result[0]\n",
    "    for i in range(1, len(full_result)):\n",
    "        d = full_result[i]\n",
    "        for el in d:\n",
    "            if i == 2:\n",
    "                waveforms[el] += -1.0 * d[el]\n",
    "            elif i == 1:\n",
    "                waveforms[el] += d[el][::-1].copy()\n",
    "            else:\n",
    "                waveforms[el] += d[el]\n",
    "    for el in waveforms:\n",
    "        waveforms[el] = waveforms[el] / len(full_result)\n",
    "\n",
    "    # Create a new `instr` in instruments list, 'instrumental' \n",
    "    if False:\n",
    "        instr = 'vocals' if 'vocals' in instruments else instruments[0]\n",
    "        if 'instrumental' not in instruments:\n",
    "            instruments.append('instrumental')\n",
    "        # Output \"instrumental\", which is an inverse of 'vocals' or the first stem in list if 'vocals' absent\n",
    "        waveforms['instrumental'] = mix_orig - waveforms[instr]\n",
    "\n",
    "    for instr in instruments:\n",
    "        estimates = waveforms[instr].T\n",
    "        if 'normalize' in config.inference:\n",
    "            if config.inference['normalize'] is True:\n",
    "                estimates = estimates * std + mean\n",
    "        file_name, _ = os.path.splitext(os.path.basename(path))\n",
    "        if False:\n",
    "            output_file = os.path.join(store_dir, f\"{file_name}_{instr}.flac\")\n",
    "            subtype = 'PCM_24'\n",
    "            sf.write(output_file, estimates, sr, subtype=subtype)\n",
    "        else:\n",
    "            output_file = os.path.join(store_dir, f\"{file_name}_{instr}.wav\")\n",
    "            sf.write(output_file, estimates, sr, subtype='FLOAT')\n",
    "\n",
    "time.sleep(1)\n",
    "print(\"Elapsed time: {:.2f} sec\".format(time.time() - start_time))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200136e5-db8c-452a-920d-b67588e0e56b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DataSphere Kernel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
